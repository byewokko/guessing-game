# Execution mode: train or test
mode: train

# Directory to load from and save to
model_dir: models

# Weights file to load: file prefix or -null- (to start with a blank model)
load_file: False

# Prefix used for saving weights result and log files
save_file: "{TIMESTAMP}-reinforce"

# Roles mode: switch or fixed
roles: switch
#roles: fixed

# Number of episodes to play in total
n_episodes: 50000

# Training batch size, also the number of episodes between updates
batch_size: 30

# Path to embedding file
#dataset: data/mcrae-484x50-vgg19.emb.gz
#dataset: data/imagenet-200x65-vgg19.train.emb.gz
dataset: data/big/mcrae-wordnet-vgg16.32bit

# Pick images based on categories (generated from the first column in the embedding file)
use_categories: True

# Number of symbols that the agents use to communicate
vocabulary_size: 100

# Size of the first layer in the agents' networks
embedding_size: 50

# Number of images presented in each turn of the game
n_active_images: 2

# Exploration strategy: none, gibbs or decay
explore: gibbs
#explore: none
#explore: decay

# Sender type: agnostic or informed
sender_type: agnostic
#sender_type: informed

# Number of CNN filters used by the informed sender
n_informed_filters: 20

# Agent loss function
# (ignored in REINFORCE algorithm)
loss: binary_crossentropy
#loss: categorical_crossentropy
#loss: mse

learning_rate: 0.01

# Batch preparation mode
# - last: only the newest memory entries are selected into the batch
# - sample: the whole memory is sampled, using the probability distribution below
batch_mode: last
#batch_mode: sample

# Probability distributions for batch sampling
# - uniform: all memory entries are given the same probability
# - linear, quadratic: probability distribution is skewed in favor of the newer memory entries
memory_sampling_distribution: linear

# Sharing experience with both agents
# Motivation: Player keeps track of his opponent's actions as well to improve his own strategy.
shared_experience: False
#shared_experience: True

# Share the first, embedding layer between the sender and receiver component within each agent
#shared_embedding: False
shared_embedding: True

# Activation function applied to the network output
#out_activation: sigmoid
out_activation: softmax  # DEFAULT

# Interrupt training early if accuracy goal (significance level) is passed
# The goal is specified in `training.py`
stop_when_goal_is_passed: False

# Model type: "old" is the standard model, other options are breaking/in-development
#model_type: old
#model_type: new
model_type: reinforce

# Save training curves in a csv dataframe
save_learning_curves: False

early_stopping: False

temperature: 10

sender_settings:
  temperature: null
  embedding_size: null
  shared_embedding: null
#  sender_type: informed
  sender_type: agnostic
  n_informed_filters: 20

receiver_settings:
  temperature: null
  embedding_size: null
  shared_embedding: null
