{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from my UU pos-tagger project (github.com/byewokko/pytorch-postagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename, padding_token=\"<PAD>\", unknown_token=\"<UNK>\"):\n",
    "    \"\"\"\n",
    "    Read text file with embeddings, return a {word: index} dict,\n",
    "    an {index: word} dict and embeddings FloatTensor\n",
    "    :param filename:\n",
    "    :return (word2ind, ind2word, embeddings):\n",
    "    \"\"\"\n",
    "    word2ind = {padding_token: 0, unknown_token: 1}\n",
    "    ind2word = {0: padding_token, 1: unknown_token}\n",
    "    embeddings = [None, None]\n",
    "\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            word, *emb_str = line.strip().split()\n",
    "            vector = [float(s) for s in emb_str]\n",
    "            if word == padding_token:\n",
    "                embeddings[0] = torch.FloatTensor(vector)\n",
    "            elif word == unknown_token:\n",
    "                embeddings[1] = torch.FloatTensor(vector)\n",
    "            else:\n",
    "                ind2word[len(word2ind)] = word\n",
    "                word2ind[word] = len(word2ind)\n",
    "                embeddings.append(torch.FloatTensor(vector))\n",
    "\n",
    "    if embeddings[0] is None:\n",
    "        embeddings[0] = torch.zeros(len(embeddings[2]))\n",
    "    if embeddings[1] is None:\n",
    "        embeddings[1] = torch.randn(len(embeddings[2]))\n",
    "\n",
    "    return word2ind, ind2word, torch.stack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingSpace():\n",
    "    \"\"\"\n",
    "    A wrapper for word embedding tensor and word-index dictionary.\n",
    "    Allows for nearest words lookup.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb_file):\n",
    "        self.word2i, self.i2word, self.emb_space = load_embeddings(emb_file)\n",
    "        self.vocab_size, self.emb_size = self.emb_space.size()\n",
    "        print(\"Vocabulary size:{:>8d}\\n\".format(self.vocab_size))\n",
    "        print(\"Embedding size: {:>8d}\\n\".format(self.emb_size))\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Returns embedding vector for a given word\n",
    "        :param key: word\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if key in self.word2i:\n",
    "            return self.emb_space[self.word2i[key]]\n",
    "        else:\n",
    "            raise KeyError(key)\n",
    "\n",
    "    def get_empty(self):\n",
    "        \"\"\"\n",
    "        Retruns an all-zeros embedding vector\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return torch.zeros(self.emb_size)\n",
    "\n",
    "    def compute_distances(self, emb_vector):\n",
    "        \"\"\"\n",
    "        Computes the cosine distance between a given vector\n",
    "        and all the words in the embedding space\n",
    "        :param emb_vector: nn.Tensor\n",
    "        :return dists: nn.Tensor\n",
    "        \"\"\"\n",
    "\n",
    "        # transform 1-dim tensor into 2-dim\n",
    "        if emb_vector.dim() == 1:\n",
    "            emb_vector = emb_vector[None, :]\n",
    "\n",
    "        # compute cosine distance using matrix multiplication\n",
    "        p_norm = emb_vector / emb_vector.norm(dim=1)[:, None]\n",
    "        s_norm = self.emb_space / self.emb_space.norm(dim=1)[:, None]\n",
    "        dists = torch.mm(p_norm, s_norm.transpose(0, 1))\n",
    "\n",
    "        return dists\n",
    "    \n",
    "    def normalize(self):\n",
    "        for dim in range(self.emb_size):\n",
    "            factor = 1/self.emb_space[dim].abs().max()\n",
    "            self.emb_space[dim] *= factor\n",
    "\n",
    "    def closest_cosine(self, emb_vector, k=10):\n",
    "        \"\"\"\n",
    "        Fetches k closest words to a given embedding vector.\n",
    "        Returns a list of (cos_distance, word) tuples.\n",
    "        :param emb_vector: nn.Tensor\n",
    "        :param k: int\n",
    "        :return results: list\n",
    "        \"\"\"\n",
    "        dists = self.compute_distances(emb_vector)\n",
    "        dist, ind = torch.topk(dists, k+1, largest=True, sorted=True)\n",
    "        return [(d.item(), self.i2word[i.item()]) for (d, i) in zip(dist[0], ind[0])][1:]\n",
    "\n",
    "    def closest_euclidean(self, emb_vector, k=10):\n",
    "        \"\"\"\n",
    "        Fetches k closest words to a given embedding vector.\n",
    "        Returns a list of (cos_distance, word) tuples.\n",
    "        :param emb_vector: nn.Tensor\n",
    "        :param k: int\n",
    "        :return results: list\n",
    "        \"\"\"\n",
    "        # transform 1-dim tensor into 2-dim\n",
    "        if emb_vector.dim() == 1:\n",
    "            emb_vector = emb_vector[None, :]\n",
    "\n",
    "        dists = (self.emb_space - emb_vector).pow(2).sum(1).sqrt()\n",
    "        \n",
    "        dist, ind = torch.topk(dists, k, largest=False, sorted=True)\n",
    "        return [(d.item(), self.i2word[i.item()]) for (d, i) in zip(dist, ind)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  400002\n",
      "\n",
      "Embedding size:       50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = \"glove.6B.50d.txt\"\n",
    "#filename = \"glove.txt\"\n",
    "es = EmbeddingSpace(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 'pure'),\n",
       " (2.851146697998047, 'essence'),\n",
       " (3.251295328140259, 'passion'),\n",
       " (3.2600531578063965, 'blend'),\n",
       " (3.2920496463775635, 'blending'),\n",
       " (3.4020566940307617, 'unadulterated'),\n",
       " (3.4189791679382324, 'purity'),\n",
       " (3.4235763549804688, 'imitation'),\n",
       " (3.4494152069091797, 'mixing'),\n",
       " (3.4566874504089355, 'perfection'),\n",
       " (3.4733824729919434, 'infused'),\n",
       " (3.50105881690979, 'mix'),\n",
       " (3.518010139465332, 'true'),\n",
       " (3.5345895290374756, 'ideal'),\n",
       " (3.56235933303833, 'â€¦'),\n",
       " (3.5777642726898193, 'purest'),\n",
       " (3.6103570461273193, 'combination'),\n",
       " (3.6242105960845947, 'imagination'),\n",
       " (3.628566265106201, 'mere'),\n",
       " (3.6636929512023926, 'genuine')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.closest_euclidean(es[\"pure\"], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(nan, '<PAD>'),\n",
       " (0.9999998211860657, 'pure'),\n",
       " (0.7904564142227173, 'essence'),\n",
       " (0.763188362121582, 'blend'),\n",
       " (0.7463705539703369, 'passion'),\n",
       " (0.7349770069122314, 'purity'),\n",
       " (0.7240849733352661, 'mix'),\n",
       " (0.7223262786865234, 'blending'),\n",
       " (0.7140454053878784, 'mixing'),\n",
       " (0.7126741409301758, 'taste'),\n",
       " (0.700041651725769, 'true'),\n",
       " (0.6979084610939026, 'unadulterated'),\n",
       " (0.6971019506454468, 'mixture'),\n",
       " (0.6926381587982178, 'kind'),\n",
       " (0.6903469562530518, 'sense'),\n",
       " (0.6832605004310608, 'ideal'),\n",
       " (0.6821432709693909, 'imagination'),\n",
       " (0.6799692511558533, 'imitation'),\n",
       " (0.6793371438980103, 'self'),\n",
       " (0.678040087223053, 'perfection')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.closest_cosine(es[\"pure\"], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(51.9129638671875, 'river'), (51.96133041381836, 'land')]\n",
      "[(49.71305847167969, 'river'), (49.75404739379883, 'land')]\n",
      "[(47.514888763427734, 'river'), (47.547828674316406, 'land')]\n",
      "[(45.318721771240234, 'river'), (45.34282684326172, 'land')]\n",
      "[(43.12485122680664, 'river'), (43.13922119140625, 'land')]\n",
      "[(40.933650970458984, 'river'), (40.937225341796875, 'land')]\n",
      "[(38.73713684082031, 'land'), (38.745567321777344, 'river')]\n",
      "[(36.53928756713867, 'land'), (36.5611686706543, 'river')]\n",
      "[(34.34410858154297, 'land'), (34.37773513793945, 'area')]\n",
      "[(32.152156829833984, 'land'), (32.18878936767578, 'area')]\n",
      "[(29.964120864868164, 'land'), (30.00419044494629, 'area')]\n",
      "[(27.78094482421875, 'land'), (27.824981689453125, 'area')]\n",
      "[(25.60386085510254, 'land'), (25.628210067749023, 'they')]\n",
      "[(23.434030532836914, 'they'), (23.434572219848633, 'land')]\n",
      "[(21.244924545288086, 'they'), (21.275461196899414, 'land')]\n",
      "[(19.062646865844727, 'they'), (19.10514259338379, 'this')]\n",
      "[(16.889842987060547, 'they'), (16.922679901123047, 'one')]\n",
      "[(14.730706214904785, 'they'), (14.749138832092285, 'one')]\n",
      "[(12.591343879699707, 'one'), (12.591569900512695, 'it')]\n",
      "[(10.449014663696289, 'right'), (10.459028244018555, 'one')]\n",
      "[(8.230388641357422, 'right'), (8.371692657470703, 'one')]\n",
      "[(6.011761665344238, 'right'), (6.373681545257568, 'one')]\n",
      "[(3.793135166168213, 'right'), (4.5069146156311035, 'but')]\n",
      "[(1.574508786201477, 'right'), (3.0874600410461426, 'but')]\n",
      "[(0.6441172957420349, 'right'), (2.5935370922088623, 'put')]\n",
      "[(2.862743616104126, 'right'), (3.2796871662139893, 'stick')]\n",
      "[(4.341229438781738, 'sticking'), (4.411965847015381, 'insistence')]\n",
      "[(5.742487907409668, 'dallies'), (5.745234966278076, '6-foot-4-inch')]\n",
      "[(6.790791034698486, 'anti-comintern'), (6.81846809387207, 'seirawan')]\n",
      "[(8.171643257141113, 'anti-comintern'), (8.20174789428711, 'bozoljac')]\n",
      "[(9.847696304321289, 'bozoljac'), (9.86312484741211, 'anti-comintern')]\n",
      "[(11.684613227844238, 'bozoljac'), (11.731639862060547, 'anti-comintern')]\n",
      "[(13.635536193847656, 'bozoljac'), (13.704964637756348, 'anti-comintern')]\n",
      "[(15.657910346984863, 'bozoljac'), (15.743741035461426, 'anti-comintern')]\n",
      "[(17.727294921875, 'bozoljac'), (17.825525283813477, 'anti-comintern')]\n",
      "[(19.828981399536133, 'bozoljac'), (19.936845779418945, 'anti-comintern')]\n",
      "[(21.953693389892578, 'bozoljac'), (22.069231033325195, 'anti-comintern')]\n",
      "[(24.095338821411133, 'bozoljac'), (24.21711540222168, 'anti-comintern')]\n",
      "[(26.249773025512695, 'bozoljac'), (26.376712799072266, 'anti-comintern')]\n",
      "[(28.414091110229492, 'bozoljac'), (28.545368194580078, 'anti-comintern')]\n",
      "[(30.586193084716797, 'bozoljac'), (30.721158981323242, 'anti-comintern')]\n",
      "[(32.764522552490234, 'bozoljac'), (32.90266799926758, 'anti-comintern')]\n",
      "[(34.947933197021484, 'bozoljac'), (35.088836669921875, 'anti-comintern')]\n",
      "[(37.13551330566406, 'bozoljac'), (37.278839111328125, 'anti-comintern')]\n",
      "[(39.326568603515625, 'bozoljac'), (39.4720344543457, 'anti-comintern')]\n",
      "[(41.52056121826172, 'bozoljac'), (41.667930603027344, 'anti-comintern')]\n",
      "[(43.71703338623047, 'bozoljac'), (43.866111755371094, 'anti-comintern')]\n",
      "[(45.91563415527344, 'bozoljac'), (46.066246032714844, 'anti-comintern')]\n",
      "[(48.1160774230957, 'bozoljac'), (48.26808166503906, 'anti-comintern')]\n",
      "[(50.318119049072266, 'bozoljac'), (50.47138595581055, 'anti-comintern')]\n"
     ]
    }
   ],
   "source": [
    "a = \"kitten\"\n",
    "b = \"cat\"\n",
    "c = \"right\"\n",
    "t = (es[a] - es[b])\n",
    "for n in np.linspace(-15,16):\n",
    "    print(es.closest_euclidean(es[c] + t*n, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
