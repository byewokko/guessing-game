# Execution mode: train or test
mode: train

# Directory to load from and save to
model_dir: models

# Weights file to load: file prefix or -null- (to start with a blank model)
load_file: False

# Prefix used for saving weights result and log files
save_file: "{TIMESTAMP}-basic"

# Roles mode: switch or fixed
#roles: switch
roles: fixed

# Number of episodes to play in total
n_episodes: 15000

# Training batch size, also the number of episodes between updates
batch_size: 30

# Path to embedding file
dataset: data/mcrae-459x100-vgg19.emb.gz
#dataset: data/imagenet-200x65-vgg19.train.emb.gz
#dataset: data/imagenet-200x15-vgg19.test-img.emb.gz
#dataset: data/imagenet-27x80-vgg19.test-ctg.emb.gz
#dataset: data/esp-10000-vgg19.emb.gz
#dataset: data/esp-10000-xception.emb.gz

# Number of images to play with (-null- to keep the whole dataset)
trim_dataset_to_n_images: False

# Pick images based on categories (generated from the first column in the embedding file)
use_categories: True

# Number of symbols that the agents use to communicate
vocabulary_size: 10

# Size of the first layer in the agents' networks
embedding_size: 50

# Number of images presented in each turn of the game
n_active_images: 2

# Exploration strategy: none, gibbs or decay
#explore: gibbs
explore: none
#explore: decay

# Gibbs temperature value
# The temperature layer is omitted if gibbs_temperature is 0
gibbs_temperature: 0

# Sender type: agnostic or informed
sender_type: agnostic
#sender_type: informed

# Number of CNN filters used by the informed sender
n_informed_filters: 20

# Agent loss function
loss: binary_crossentropy
#loss: categorical_crossentropy
#loss: mse

#learning_rate: 0.01

# Batch preparation mode
# - last: only the newest memory entries are selected into the batch
# - sample: the whole memory is sampled, using the probability distribution below
batch_mode: last
#batch_mode: sample

# Probability distributions for batch sampling
# - uniform: all memory entries are given the same probability
# - linear, quadratic: probability distribution is skewed in favor of the newer memory entries
memory_sampling_distribution: linear

# Dropout applied after first (embedding) layer
# (doesn't make difference)
dropout: 0

# Sharing experience with both agents
# Motivation: Player keeps track of his opponent's actions as well to improve his own strategy.
shared_experience: False
#shared_experience: True

# Share the first, embedding layer between the sender and receiver component within each agent
shared_embedding: False
#shared_embedding: True

# Activation function applied to the network output
#out_activation: sigmoid
out_activation: softmax  # DEFAULT

# Interrupt training early if accuracy goal (significance level) is passed
# The goal is specified in `training.py`
stop_when_goal_is_passed: False

# Model type: "old" is the standard model, other options are breaking/in-development
#model_type: old
#model_type: new
model_type: reinforce

# Save training curves in a csv dataframe
save_learning_curves: False

early_stopping: False
