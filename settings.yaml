# Execution mode: train or test
mode: train

# Directory to load from and save to
model_dir: models

# Weights file to load: file prefix or -null- (to start with a blank model)
load_file: False

# Prefix used for saving weights result and log files
save_file: "{TIMESTAMP}-sharedexp"

# Roles mode: switch or fixed
#roles: switch
roles: fixed

# Number of episodes to play in total
n_episodes: 1000

# Training batch size, also the number of episodes between updates
batch_size: 30

# Path to embedding file
#dataset: data/imagenet-4000-vgg19.emb.gz
dataset: data/imagenet-200x65-vgg19.train.emb.gz
#dataset: data/imagenet-200x15-vgg19.test-img.emb.gz
#dataset: data/imagenet-27x80-vgg19.test-ctg.emb.gz
#dataset: data/esp-10000-vgg19.emb.gz
#dataset: data/esp-10000-xception.emb.gz

# Number of images to play with (-null- to keep the whole dataset)
trim_dataset_to_n_images: False

# Pick images based on categories (generated from the first column in the embedding file)
use_categories: True

# Number of symbols that the agents use to communicate
vocabulary_size: 10

# Size of the first layer in the agents' networks
embedding_size: 50

# Number of images presented in each turn of the game
n_active_images: 4

# Exploration strategy: False gibbs or decay
explore: gibbs
#explore: False
#explore: decay

# Gibbs temperature value
gibbs_temperature: 0.1

# Sender type: agnostic or informed
#sender_type: agnostic
sender_type: informed

# Number of CNN filters used by the informed sender
n_informed_filters: 20

# Agent loss function
loss: binary_crossentropy
#loss: categorical_crossentropy
#loss: mse

#learning_rate: 0.01

# Batch preparation mode
# - last: only the newest memory entries are selected into the batch
# - sample: the whole memory is sampled, using the probability distribution below
#batch_mode: last
batch_mode: sample

# Probability distributions for batch sampling
# - uniform: all memory entries are given the same probability
# - linear, quadratic: probability distribution is skewed in favor of the newer memory entries
memory_sampling_distribution: linear

# Dropout applied after first layer
# (doesn't make difference)
dropout: 0

# Sharing experience with both components of each agent
shared_experience: False
#shared_experience: True

# Share the first, embedding layer between the sender and receiver component within each agent
#shared_embedding: False
shared_embedding: True

# Activation function applied to the network output
#out_activation: sigmoid
out_activation: softmax  # DEFAULT

# Interrupt training early if accuracy goal is passed
stop_when_goal_is_passed: True